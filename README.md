❤️ Lovely JAX
================

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

## Read full docs [here](https://xl0.github.io/lovely-tensors/). PyTorch version [here](https://github.com/xl0/lovely-tensors)

## Install

``` sh
pip install lovely-jax
```

## How to use

How often do you find yourself debugging JAX code? You dump an array to
the cell output, and see this:

``` python
numbers
```

    Array([[[-0.354, -0.337, -0.405, ..., -0.56 , -0.474,  2.249],
            [-0.405, -0.423, -0.491, ..., -0.919, -0.851,  2.163],
            [-0.474, -0.474, -0.542, ..., -1.039, -1.039,  2.198],
            ...,
            [-0.902, -0.834, -0.936, ..., -1.467, -1.296,  2.232],
            [-0.851, -0.782, -0.936, ..., -1.604, -1.501,  2.18 ],
            [-0.834, -0.816, -0.971, ..., -1.656, -1.553,  2.112]],

           [[-0.197, -0.197, -0.303, ..., -0.478, -0.373,  2.411],
            [-0.25 , -0.232, -0.338, ..., -0.705, -0.67 ,  2.359],
            [-0.303, -0.285, -0.39 , ..., -0.74 , -0.81 ,  2.376],
            ...,
            [-0.425, -0.232, -0.373, ..., -1.09 , -1.02 ,  2.429],
            [-0.39 , -0.232, -0.425, ..., -1.23 , -1.23 ,  2.411],
            [-0.408, -0.285, -0.478, ..., -1.283, -1.283,  2.341]],

           [[-0.672, -0.985, -0.881, ..., -0.968, -0.689,  2.396],
            [-0.724, -1.072, -0.968, ..., -1.247, -1.02 ,  2.326],
            [-0.828, -1.125, -1.02 , ..., -1.264, -1.16 ,  2.379],
            ...,
            [-1.229, -1.473, -1.386, ..., -1.508, -1.264,  2.518],
            [-1.194, -1.456, -1.421, ..., -1.648, -1.473,  2.431],
            [-1.229, -1.526, -1.508, ..., -1.682, -1.526,  2.361]]], dtype=float32)

Was it really useful for you, as a human, to see all these numbers?

What is the shape? The size?  
What are the statistics?  
Are any of the values `nan` or `inf`?  
Is it an image of a man holding a tench?

``` python
import lovely_jax as lj
```

``` python
lj.monkey_patch()
```

## `__repr__`

``` python
numbers # torch.Tensor
```

    Array[3, 196, 196] n=115248 x∈[-2.118, 2.640] μ=-0.388 σ=1.073

Better, huh?

``` python
numbers[1,:6,1] # Still shows values if there are not too many.
```

    Array[6] x∈[-0.443, -0.197] μ=-0.311 σ=0.083 [-0.197, -0.232, -0.285, -0.373, -0.443, -0.338]

``` python
spicy = numbers.flatten()[:12].copy()

spicy = (spicy  .at[0].mul(10000)
                .at[1].divide(10000)
                .at[2].set(float('inf'))
                .at[3].set(float('-inf'))
                .at[4].set(float('nan'))
                .reshape((2,6)))
spicy # Spicy stuff
```

    Array[2, 6] n=12 x∈[-3.541e+03, -3.369e-05] μ=-393.776 σ=1.113e+03 +inf! -inf! nan!

``` python
jnp.zeros((10, 10)) # A zero tensor - make it obvious
```

    Array[10, 10] n=100 all_zeros

``` python
spicy.v # Verbose
```

    Array[2, 6] n=12 x∈[-3.541e+03, -3.369e-05] μ=-393.776 σ=1.113e+03 +inf! -inf! nan!
    Array([[-3.541e+03, -3.369e-05,        inf,       -inf,        nan, -4.054e-01],
           [-4.226e-01, -4.911e-01, -5.082e-01, -5.596e-01, -5.424e-01, -5.082e-01]], dtype=float32)

``` python
spicy.p # The plain old way
```

    Array([[-3.541e+03, -3.369e-05,        inf,       -inf,        nan, -4.054e-01],
           [-4.226e-01, -4.911e-01, -5.082e-01, -5.596e-01, -5.424e-01, -5.082e-01]], dtype=float32)

## Going `.deeper`

``` python
numbers.deeper
```

    Array[3, 196, 196] n=115248 x∈[-2.118, 2.640] μ=-0.388 σ=1.073
      Array[196, 196] n=38416 x∈[-2.118, 2.249] μ=-0.324 σ=1.036
      Array[196, 196] n=38416 x∈[-1.966, 2.429] μ=-0.274 σ=0.973
      Array[196, 196] n=38416 x∈[-1.804, 2.640] μ=-0.567 σ=1.178

``` python
# You can go deeper if you need to
numbers[:,:3,:5].deeper(2)
```

    Array[3, 3, 5] n=45 x∈[-1.316, -0.197] μ=-0.593 σ=0.302
      Array[3, 5] n=15 x∈[-0.765, -0.337] μ=-0.492 σ=0.119
        Array[5] x∈[-0.440, -0.337] μ=-0.385 σ=0.037 [-0.354, -0.337, -0.405, -0.440, -0.388]
        Array[5] x∈[-0.662, -0.405] μ=-0.512 σ=0.097 [-0.405, -0.423, -0.491, -0.577, -0.662]
        Array[5] x∈[-0.765, -0.474] μ=-0.580 σ=0.112 [-0.474, -0.474, -0.542, -0.645, -0.765]
      Array[3, 5] n=15 x∈[-0.513, -0.197] μ=-0.321 σ=0.096
        Array[5] x∈[-0.303, -0.197] μ=-0.243 σ=0.049 [-0.197, -0.197, -0.303, -0.303, -0.215]
        Array[5] x∈[-0.408, -0.232] μ=-0.327 σ=0.075 [-0.250, -0.232, -0.338, -0.408, -0.408]
        Array[5] x∈[-0.513, -0.285] μ=-0.394 σ=0.091 [-0.303, -0.285, -0.390, -0.478, -0.513]
      Array[3, 5] n=15 x∈[-1.316, -0.672] μ=-0.964 σ=0.170
        Array[5] x∈[-0.985, -0.672] μ=-0.846 σ=0.110 [-0.672, -0.985, -0.881, -0.776, -0.916]
        Array[5] x∈[-1.212, -0.724] μ=-0.989 σ=0.160 [-0.724, -1.072, -0.968, -0.968, -1.212]
        Array[5] x∈[-1.316, -0.828] μ=-1.058 σ=0.160 [-0.828, -1.125, -1.020, -1.003, -1.316]

``` python
# __tracebackhide__=False

# def f(x):
#     __tracebackhide__=False

#     jax.debug.print(" sdfs {x} sfsdf", x=x)
#     jax.debug.print(" sdfs {x} sfsdf", x=type(x))
#     return x*2

# fj = jax.jit(f)

# _ = fj(numbers)


# # print(numbers)
```

``` python
print(repr(numbers))
```

    Array[3, 196, 196] n=115248 x∈[-2.118, 2.640] μ=-0.388 σ=1.073

## Without `.monkey_patch`

``` python
lj.lovely(spicy)
```

    Array[2, 6] n=12 x∈[-3.541e+03, -3.369e-05] μ=-393.776 σ=1.113e+03 +inf! -inf! nan!

``` python
lj.lovely(spicy, verbose=True)
```

    Array[2, 6] n=12 x∈[-3.541e+03, -3.369e-05] μ=-393.776 σ=1.113e+03 +inf! -inf! nan!
    Array([[-3.541e+03, -3.369e-05,        inf,       -inf,        nan, -4.054e-01],
           [-4.226e-01, -4.911e-01, -5.082e-01, -5.596e-01, -5.424e-01, -5.082e-01]], dtype=float32)

``` python
lj.lovely(numbers, depth=1)
```

    Array[3, 196, 196] n=115248 x∈[-2.118, 2.640] μ=-0.388 σ=1.073
      Array[196, 196] n=38416 x∈[-2.118, 2.249] μ=-0.324 σ=1.036
      Array[196, 196] n=38416 x∈[-1.966, 2.429] μ=-0.274 σ=0.973
      Array[196, 196] n=38416 x∈[-1.804, 2.640] μ=-0.567 σ=1.178
